---
title: "Databricks Connect"
output: html_notebook
---

```{r setup}
# Packages ----
library(dplyr)
library(sparklyr)
library(ggplot2)
```

## Connecting
If necessary, install Spark
```{r}
# sparklyr::spark_install(version = "2.4.4", hadoop_version = "2.7")
```

```{r spark-connection}
config <- spark_config()
# config$spark.driver.memory = "2g"
# config$sql.catalogImplementation = "hive"
sc <- spark_connect(method = "databricks", config = config)
```

```{r}
sc
```

## Copying data
Data that is copied into DB using the cluster web interface, then turned into a
table
```{r write-data}
fs::dir_create("data")
readr::write_csv(nycflights13::flights, "data/flights.csv")
readr::write_csv(iris, "data/iris.csv")
```

```{r}
cars <- copy_to(sc, mtcars, "cars", overwrite = TRUE)
```

```{r}
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
```


```{r}
sdf_len(sc, 10) %>% 
  sdf_register("nums")

nums <- tbl(sc, "nums")
nums
```


```{r}
cars <- sdf_copy_to(sc, mtcars, serializer = "csv_string", overwrite = TRUE)
```

```{r}
cars <- sdf_copy_to(sc, mtcars, overwrite = TRUE)
```


```{r}
sparklyr:::spark_connection_in_driver(sc)
```

```{r}
sparklyr:::spark_data_perform_copy(sc, sparklyr:::spark_serialize_csv_string, mtcars, -1)
```

There are managed and unmanaged tables in Spark (https://docs.databricks.com/data/tables.html#managed-and-unmanaged-tables). This showcases how to create managed and unmanaged tables from Spark DataFrames.

```{r}
retail <- spark_read_csv(sc, path = "dbfs:/databricks-datasets/online_retail/data-001/data.csv", name = "retail")
```

```{r}
retail
```


### Managed Tables
For this table, Spark manages both the data and the metadata. The metadata and data are both stored in dbfs. Dropping this table deletes both metadata and the actual data.
```{r}
invoice_summary <- retail %>% 
  mutate(total = Quantity * UnitPrice) %>% 
  group_by(InvoiceNo) %>% 
  summarise(Total = sum(total, na.rm = TRUE))
```

```{r}
invoice_summary %>% 
  spark_write_table("invoice_summary", mode = "overwrite")
```

### Unmanaged tables
For this table, Spark manages the metadata while the data location is specifically specified. Dropping the table only deletes the metadata and does not impact the actual data.

```{r}
customer_summary <- retail %>% 
  mutate(total = UnitPrice * Quantity) %>% 
  group_by(CustomerID) %>% 
  summarise(Total = sum(total, na.rm = TRUE))
```

It looks like these tables always save as parquet files in a directory defined in options(path = <path>). Can we specify an alternative format? 
```{r}
customer_summary %>% 
  spark_write_table(name = "customer_summary", 
                    mode = "overwrite", 
                    options = list(
                      path = "dbfs:/retail/customer-summary"))
```

## Connecting to data
Connecting to previous tables
```{r}
customer_summary_p <- spark_read_parquet(sc, path = "dbfs:/retail/customer-summary.parquet")
```



```{r spark-data}
flights <- tbl(sc, "flights")
```

```{r}
head(flights)
```

```{r}
flights %>% 
  count(year)
```

## ML in Spark
Taken from: https://spark.rstudio.com/mlib/
```{r}
iris_tbl <- tbl(sc, "iris")
```

```{r}
kmeans_model <- iris_tbl %>% 
  ml_kmeans(Species ~ ., k = 3)
```

```{r}
ml_predict(kmeans_model) %>%
  collect() %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
             size = 2, alpha = 0.5) + 
  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
             col = scales::muted(c("red", "green", "blue")),
             pch = 'x', size = 12) +
  scale_color_discrete(name = "Predicted Cluster",
                       labels = paste("Cluster", 1:3)) +
  labs(
    x = "Petal Length",
    y = "Petal Width",
    title = "K-Means Clustering",
    subtitle = "Spark overkill"
  )
```


```{r}
lm_model <- iris_tbl %>% 
  select(Petal_Width, Petal_Length) %>% 
  ml_linear_regression(Petal_Length ~ Petal_Width)
```

## Disconnect
```{r}
spark_disconnect(sc)
```

